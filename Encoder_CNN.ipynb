{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    image_path = 'dataset'\n",
    "    sub_dir = os.listdir(image_path)\n",
    "    path = [image_path+'/'+str(sub)+'/'+str(sub_path) for sub in sub_dir for sub_path in os.listdir(image_path+'/'+str(sub))]\n",
    "    return(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_vec(path):\n",
    "    for sample in path[:5]:\n",
    "        img = Image.open(sample).convert('RGBA')\n",
    "        arr = np.array(img) #(Batch, Height, Width, Depth)\n",
    "        # print(arr.shape)  Original Size        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(path):\n",
    "    image = []\n",
    "    for i in path:\n",
    "        img = cv2.imread(i, cv2.IMREAD_UNCHANGED) \n",
    "        \n",
    "        ### Resize ###\n",
    "        height = 220\n",
    "        width = 220\n",
    "        dim = (width, height)\n",
    "        res = cv2.resize(img, dim, interpolation=cv2.INTER_LINEAR)\n",
    "        res = res.reshape(3, 220, 220)\n",
    "        \n",
    "        ### Gaussian Smoothening ###\n",
    "        blur = cv2.GaussianBlur(res, (5,5), 0)\n",
    "        image.append(blur)\n",
    "        \n",
    "        # Final shape [batch_size, input_channels, height, width] \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        #---------------#\n",
    "        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=4, stride=1, padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(8)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size = 2)\n",
    "        #---------------#\n",
    "        self.cnn2 = nn.Conv2d(in_channels=8, out_channels=32, kernel_size=6, stride=1, padding = 2)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size = 2)\n",
    "        #---------------#\n",
    "        self.fc1 = nn.Linear(in_features = 466560, out_features = 600)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(in_features = 600, out_features = 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool2(out)\n",
    "        print(out.shape)\n",
    "        out = out.view(-1, 466560)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(inputs):\n",
    "    model = EncoderCNN()\n",
    "    outputs = model(inputs)\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    path = read_data()\n",
    "    pre_image = torch.tensor(image_preprocessing(path[:5]), dtype=torch.float)\n",
    "    print(pre_image.shape)\n",
    "    run_model(pre_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 220, 220])\n",
      "torch.Size([5, 32, 54, 54])\n",
      "tensor([[-0.2072,  0.1306, -0.0814, -0.2503, -0.0023, -0.0441, -0.5391, -0.1127,\n",
      "         -0.1916,  0.1281]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
