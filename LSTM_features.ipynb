{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers = 1):\n",
    "        super(DecoderRNN, self).__init__() \n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embed_size)\n",
    "        self.linear1 = nn.Linear(embed_size*14, hidden_size)\n",
    "        #self.lstm = nn.LSTM(input_size = embed_size, hidden_size = hidden_size, batch_first = True)\n",
    "        ### Can add another lstm ###\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, captions):\n",
    "        embed = self.embedding_layer(captions)\n",
    "        \n",
    "        # embed = torch.cat((features.unsqueeze(1), embed), dim = 1)\n",
    "        #lstm_outputs, _ = self.lstm(embed)\n",
    "        \n",
    "        out = F.relu(self.linear1(embed))\n",
    "        out = self.linear2(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_paths():\n",
    "    dir_ , file_name = [], []\n",
    "    datadir = 'sentence'\n",
    "    subdirs = [x[0] for x in os.walk(datadir)][1:3] # remove curr folder from list\n",
    "    textdir = [x[2:] for sub in subdirs for x in os.walk(sub)] # remove empty folder\n",
    "    for i,x in enumerate(textdir):\n",
    "        sub = subdirs[i]\n",
    "        file_name += [data for data in x[0]]\n",
    "        comb_dir = [sub+'/'+data for data in x[0]]\n",
    "        dir_ += [d for d in comb_dir]\n",
    "    return(dir_, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(paths, files):\n",
    "    lines, tokens, data, file_name = [], [], [], []\n",
    "    for i,path in enumerate(paths):\n",
    "        \n",
    "        context = [sent.replace('\\n','') for sent in open(path).readlines()[:-1]] \n",
    "        target = [open(path).readlines()[-1].replace('\\n', '')]        \n",
    "        \n",
    "        c_tokens = [sent.split() for sent in context]\n",
    "        t_tokens = [sent.split() for sent in target]\n",
    "        \n",
    "        tokens += c_tokens + t_tokens\n",
    "        \n",
    "        data.append((c_tokens, t_tokens))\n",
    "        file_name.append(files[i])\n",
    "        \n",
    "    words = [w for sent in tokens for w in sent]\n",
    "    word_to_idx = {w:i for i, w in enumerate(words)}\n",
    "    word_to_idx['UNK'] = len(word_to_idx)\n",
    "    return(data, file_name, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_data(data, file_name):\n",
    "    data = data[:5]\n",
    "    for i, (c,t) in enumerate(data):\n",
    "        print(\"The file is : {}\". format(file_name[i]))\n",
    "        print(\"The context data is : {}\".format(c))\n",
    "        print(\"The target data is : {}\".format(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(data, word_to_idx):\n",
    "    train_data, len_c, len_t = [], [], []\n",
    "    for c, t in data:\n",
    "        context_data = []\n",
    "        for sent in c:\n",
    "            add_unk = ['UNK'] * (27 - len(sent)) # 27 is max_len\n",
    "            sent += add_unk\n",
    "            context = [ word_to_idx[word] for word in sent] \n",
    "            context_data.append(context)\n",
    "        len_c.append(len(context))\n",
    "        \n",
    "        add_unk = ['UNK']*(18-len(t)) # 18 is max_len\n",
    "        target = [ word_to_idx[word] for sent in t for word in sent+add_unk]\n",
    "        len_t.append(len(target))\n",
    "        max_t = max(len_t)\n",
    "        \n",
    "        for c in context_data: # Make the last sentence the target for all the others\n",
    "            train_data.append((c, target))\n",
    "            \n",
    "    max_c = max(len_c)\n",
    "    return(train_data, max_c, max_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data):\n",
    "    num_epochs = 10\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for context, target in (data):\n",
    "            print(\"The context is : {}\".format(context))\n",
    "            print(\"The target is : {}\".format(target))\n",
    "            outputs = model(torch.tensor([context], dtype = torch.long))\n",
    "            loss = loss_func(outputs, torch.tensor([target], dtype = torch.long))\n",
    "        \"\"\"    \n",
    "            total_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(total_loss)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    paths, file_name = read_paths()\n",
    "    text_data, file_name, word_to_idx = read_data(paths, file_name)\n",
    "    view_data(text_data, file_name)\n",
    "    vec_data, max_c, max_t = create_data(text_data, word_to_idx)\n",
    "    print(len(word_to_idx))\n",
    "    EMBEDDING_DIM = 128\n",
    "    HIDDEN_DIM = 2\n",
    "    \n",
    "    #model = DecoderRNN(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_idx), num_layers = 1)\n",
    "    #train(model, vec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file is : 2008_000912txt\n",
      "The context data is : [['A', 'close-up', 'of', 'a', 'brown', 'horse.'], ['A', 'horse', 'facing', 'the', 'camera', 'with', 'a', 'horse', 'in', 'the.background'], ['A', 'horse', 'standing', 'near', 'a', 'gate', 'with', 'another', 'horse', 'in', 'the', 'background.'], ['A', 'reddish', 'brown', 'horse', 'looking', 'over', 'a', 'fence.']]\n",
      "The target data is : [['A', 'young', 'horse', 'looks', 'into', 'the', 'camera', 'from', 'behind', 'a', 'fence.']]\n",
      "The file is : 2008_008262txt\n",
      "The context data is : [['A', 'black', 'and', 'white', 'photo', 'of', 'three', 'horses,', 'their', 'handlers,', 'and', 'three', 'onlookers.'], ['Black', 'and', 'white', 'scene', 'of', 'three', 'people', 'looking', 'at', 'three', 'horses.'], ['People', 'stand', 'in', 'the', 'arena', 'with', 'horses.'], ['The', 'horses', 'prepare', 'for', 'the', 'show.']]\n",
      "The target data is : [['Three', 'horses', 'are', 'shown', 'using', 'halters.']]\n",
      "The file is : 2008_003447txt\n",
      "The context data is : [['A', 'horse', 'galloping', 'while', 'wearing', 'the', 'number', '4.'], ['A', 'race', 'horse', 'galloping', 'without', 'a', 'rider.'], ['Brown', 'race', 'horse', 'wearing', 'race', 'mask', 'and', 'halter.'], ['Horse', 'in', 'a', 'competition.']]\n",
      "The target data is : [['The', 'racing', 'horse', 'is', 'number', 'four.']]\n",
      "The file is : 2008_008393txt\n",
      "The context data is : [['A', 'close', 'up', 'of', 'a', 'horse', 'wearing', 'a', 'blue', 'halter.'], ['An', 'upclose', 'shot', 'of', 'a', 'horse', 'in', 'the', 'barn.'], ['An', 'up', 'close', 'view', 'of', 'a', 'horse.'], ['Close', 'up', 'of', 'a', 'brown', 'horse', 'with', 'a', 'blue', 'halter.']]\n",
      "The target data is : [['Up', 'close', 'view', 'of', 'horse', 'with', 'halter', 'attached', 'to', 'face.']]\n",
      "The file is : 2008_004868txt\n",
      "The context data is : [['a', 'bald', 'man', 'smiling', 'with', 'a', 'black,', 'white-haired', 'horse'], ['A', 'bald', 'man', 'touches', 'the', 'nose', 'of', 'a', 'black', 'horse', 'on', 'the', 'other', 'side', 'of', 'a', 'fence.'], ['A', 'bald', 'man', 'with', 'a', 'beard', 'pets', 'the', 'nose', 'of', 'a', 'black', 'and', 'white', 'horse', 'at', 'the', 'fence.'], ['A', 'man', 'is', 'staring', 'at', 'a', 'horse', 'with', 'a', 'white', 'mane.']]\n",
      "The target data is : [['A', 'man', 'strokes', 'a', 'horses', 'nose', 'while', 'it', 'stands', 'over', 'a', 'fence.']]\n",
      "853\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
